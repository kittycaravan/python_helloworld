"""
[0/7]  임포트
"""
import tensorflow as tf

# TensorFlow의 Keras API에서 제공하는 텍스트 전처리 도구 중 하나인 Tokenizer 클래스를 가져오는 구문
# 이 클래스는 주어진 텍스트 데이터를 토큰화하고, 각 토큰에 고유한 정수 인덱스를 할당합니다. 
# 이는 텍스트 데이터를 신경망 모델에 입력으로 제공하기 전에 수행되는 일반적인 전처리 과정 중 하나입니다. 
# 토큰화된 텍스트는 후속 단계에서 Embedding 레이어에 입력으로 제공되거나 
# 다른 형태의 텍스트 분석에 사용될 수 있습니다.
#
# 간단히 말해, Tokenizer 클래스는 텍스트를 단어 단위로 분할하고 각 단어를 정수 인덱스로 
# 매핑하는 데 사용됩니다. 이는 텍스트 데이터를 다루고 신경망 모델에 입력하기 전에 필요한 
# 전처리 과정 중 하나입니다.
from tensorflow.keras.preprocessing.text import Tokenizer

# TensorFlow의 Keras API에서 제공하는 시퀀스 패딩 도구인 pad_sequences 함수를 가져오는 구문입니다.
# 이 함수는 시퀀스 데이터(예: 텍스트 문장, 시계열 데이터 등)의 길이를 
# 일정하게 만들기 위해 사용됩니다. 
# 주어진 시퀀스 데이터의 길이가 다를 경우, 패딩 과정을 통해 모든 시퀀스를 동일한 길이로 맞춥니다. 
# 이를 통해 신경망 모델에 입력으로 주어질 데이터의 형태를 일관되게 만들어 줍니다.
# 패딩은 주로 시퀀스 데이터의 뒷부분에 0 또는 다른 특정 값으로 채워집니다. 
# 이러한 패딩된 시퀀스는 후속 단계에서 신경망 모델에 입력으로 사용됩니다.

# 간단히 말해, pad_sequences 함수는 시퀀스 데이터의 길이를 일정하게 맞춰주는데 사용되며, 
# 주로 텍스트 데이터나 시계열 데이터를 다룰 때 활용됩니다.
from tensorflow.keras.preprocessing.sequence import pad_sequences

import numpy as np

"""
[1/7] 학습할 자료 준비
"""
# 긍정적인 문장들
positive_sentences = [
    "밥이 맛있어요",
    "맛집",
    "맛집 추천함",
    "맛집이라 자주 가요",
    "우리동네 맛집임",
    "동네 최고의 집",
    "동네에서 잘 안시켜먹는데 여긴 자주 주문해요",
    "드디어 우리 동네에도!",
    "좋아요",
    "우리 고양이도 잘 먹어요"
]

# 부정적인 문장들
negative_sentences = [
    "별로에요",
    "쓰레기",
    "짜증나네요",
    "밥이 질어요"
]

# 레이블 (0: 부정, 1: 긍정)
# labels = [1,1,1,1,1,1,1,1,1,1,0,0,0,0]
labels = [1] * len(positive_sentences) + [0] * len(negative_sentences)

"""
[2/7] 데이터 가공
"""
# Tokenizer 객체 생성 및 텍스트 시퀀스를 정수 시퀀스로 변환
tokenizer = Tokenizer()

# Tokenizer 객체를 사용하여 주어진 문장 리스트(긍정적인 문장들과 부정적인 문장들을 합친 것)에 
# 대해 텍스트를 토큰화하고, 각 토큰에 대해 고유한 정수 인덱스를 생성하는 메서드
tokenizer.fit_on_texts(positive_sentences + negative_sentences)

# 이 줄의 주요 역할은 Tokenizer 객체를 사용하여 텍스트 문장을 정수 시퀀스로 변환하는 것입니다.
# 각 문장은 텍스트 데이터로 구성되어 있으며, 
# Tokenizer의 texts_to_sequences 메서드를 사용하여 텍스트 문장을 정수로 변환된 시퀀스로 바꿉니다. 
# 이는 신경망 모델의 입력으로 사용하기 위한 전처리 과정 중 하나입니다.
# 여기서 positive_sentences + negative_sentences는 
# 긍정적인 문장과 부정적인 문장을 합친 리스트입니다. 
# 이렇게 함으로써 Tokenizer는 모든 문장을 고려하여 텍스트를 정수로 변환하는 데 사용됩니다.
# 변환된 시퀀스는 sequences 변수에 저장되며, 
# 이 시퀀스는 패딩된 시퀀스를 만들기 위한 후속 단계에서 사용됩니다.
sequences = tokenizer.texts_to_sequences(positive_sentences + negative_sentences)

## 패딩된 시퀀스 생성

# 이 코드는 시퀀스 중에서 가장 긴 시퀀스의 길이를 찾는 역할을 합니다.
# sequences 변수에는 텍스트 문장을 정수 시퀀스로 변환한 결과가 저장되어 있습니다. 
# 각 시퀀스의 길이는 각 문장의 단어 수에 해당합니다. 
# 따라서 이 코드는 리스트 컴프리헨션을 사용하여 sequences 리스트에 있는 각 시퀀스의 길이를 계산하고,
# 그 중에서 가장 긴 시퀀스의 길이를 찾아내기 위해 max 함수를 사용합니다.
# 그리고 이를 max_len 변수에 할당하여 후속 단계에서 패딩할 때 사용합니다. 
# 패딩된 시퀀스는 모든 시퀀스가 동일한 길이를 갖도록 만들어주는데, 
# 이 때 사용할 최대 시퀀스 길이를 max_len으로 설정합니다.
max_len = max([len(seq) for seq in sequences])

# 이 코드는 시퀀스를 패딩하여 모든 시퀀스가 동일한 길이를 갖도록 만드는 역할을 합니다.
# pad_sequences 함수는 시퀀스 데이터의 길이를 일정하게 만들기 위해 사용됩니다. 
# 이 함수는 주어진 시퀀스 데이터의 길이가 다를 경우, 
# 시퀀스의 뒷부분에 패딩을 추가하여 모든 시퀀스의 길이를 동일하게 만듭니다.
# 여기서 sequences는 정수로 변환된 텍스트 시퀀스가 저장된 리스트이고, 
# max_len은 패딩할 시퀀스의 최대 길이입니다. 
# padding 매개변수는 패딩을 추가하는 위치를 결정하는데, 
# 'post'로 설정되어 있으므로 시퀀스의 뒷부분에 패딩이 추가됩니다.
# 따라서 padded_sequences 변수에는 모든 시퀀스가 max_len의 길이로 패딩되어 저장됩니다. 
# 이 패딩된 시퀀스는 후속 단계에서 신경망 모델에 입력으로 제공됩니다.
padded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post')

# 레이블을 넘파이 배열로 변환
labels = np.array(labels)

"""
[3/7] 모델 구성 (감정 분석)
"""
# 아래 코드는 감정 분석을 위한 간단한 신경망 모델을 생성하는 부분입니다.
#
# Embedding 레이어: 
# 이 레이어는 텍스트 데이터를 밀집된 벡터 형태로 변환해줍니다. 
# 각 단어는 고유한 정수로 매핑되며, 이 정수에 해당하는 임베딩 벡터로 변환됩니다. 
# 이 벡터는 학습 과정에서 업데이트되며, 단어 간의 의미적 유사성을 고려하여 배치됩니다. 
# input_dim은 전체 단어 집합의 크기(단어의 총 개수 + 1)이고, 
# output_dim은 임베딩 벡터의 차원입니다.
#
# GlobalAveragePooling1D 레이어: 
# 이 레이어는 시퀀스 데이터의 길이를 고려하여 각 시퀀스에 대한 평균을 계산합니다. 
# 이를 통해 시퀀스의 길이에 상관없이 일정한 크기의 출력이 생성됩니다. 
# 이 경우, 각 시퀀스의 임베딩 벡터에 대한 평균이 계산됩니다.
# 
# <차원> : 일반적으로 임베딩 벡터의 차원이 클수록 단어의 특성을 더욱 세밀하게 표현할 수 있지만, 
# 모델의 복잡도가 증가하고 학습 시간이 늘어날 수 있습니다.
#
#
# Dense 레이어: 이 레이어는 완전 연결층입니다. 
# 이전 레이어에서 얻은 특성을 기반으로 감정을 예측하기 위해 사용됩니다. 
# 이진 분류 문제이므로 하나의 뉴런만 있는 출력 레이어로 설정되었으며, 
# 시그모이드 활성화 함수를 사용하여 출력이 0과 1 사이의 확률로 변환됩니다. 
# 0.5 이상이면 긍정으로, 0.5 미만이면 부정으로 분류됩니다.
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=16),
    tf.keras.layers.GlobalAveragePooling1D(),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

"""
[4/7] 모델 컴파일
"""
# optimizer='adam': 옵티마이저는 모델이 사용하는 최적화 알고리즘을 정의합니다. 
# 'adam'은 Adam 최적화 알고리즘을 사용하겠다는 것을 의미합니다. 
# Adam은 경사 하강법의 한 종류로, 학습 중에 모델의 가중치를 업데이트하는 데 사용됩니다. 
# Adam은 자동으로 학습률을 조절하면서 모멘텀과 RMSprop을 결합하여 효율적으로 학습합니다.
#
# loss='binary_crossentropy': 손실 함수는 모델이 학습하는 동안 사용되는 측정 항목입니다. 
# 여기서는 이진 분류 문제이므로 'binary_crossentropy' 손실 함수를 사용합니다. 
# 이 함수는 이진 분류에서 주로 사용되며, 신경망의 출력과 실제 레이블 간의 교차 엔트로피를 계산합니다. 
# 모델이 예측과 실제 레이블 간의 차이를 최소화하도록 이 손실 함수를 최적화합니다.
#
# metrics=['accuracy']: 메트릭은 모델의 성능을 평가하는 데 사용되는 지표입니다. 
# 여기서는 'accuracy'(정확도) 메트릭을 사용합니다. 
# 이 메트릭은 모델이 올바른 예측을 몇 개나 만들었는지를 나타내며, 
# 이진 분류에서 가장 일반적으로 사용되는 지표 중 하나입니다. 
# 모델이 예측한 레이블과 실제 레이블 간의 일치율을 측정합니다.
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

"""
[5/7] 모델 훈련
"""
# padded_sequences: 훈련에 사용할 패딩된 시퀀스 데이터입니다. 
# 이 데이터는 텍스트 문장을 정수 시퀀스로 변환하고, 최대 길이에 맞춰서 패딩된 형태입니다.

# labels: 각 패딩된 시퀀스 데이터에 대한 레이블입니다. 
# 이진 분류 문제에서는 긍정(positive) 또는 부정(negative) 클래스를 나타내는 0 또는 1의 값을 가집니다.

# epochs=10: 에포크(epoch)는 전체 훈련 데이터셋을 한 번 모델에 통과시키는 것을 의미합니다. 
# 이 매개변수는 전체 데이터셋을 몇 번 반복하여 학습할지를 결정합니다. 
# 에포크가 높을수록 모델은 더 많은 학습을 수행하게 되지만, 
# 너무 높게 설정하면 과적합(overfitting)이 발생할 수 있습니다. 
# 10번의 에포크는 전체 데이터셋을 10번 반복하여 학습하겠다는 것을 의미합니다.
model.fit(padded_sequences, labels, epochs=10)


"""
[6/7] 새로운 문장에 대한 감정 분석 예측
"""
# 참고 : 토큰화 과정에서 일반적으로 공백을 기준으로 단어를 분리함.


# 우리: 1
# 동네에도: 2
# 드디어: 3
# 먹을만한: 4
# 곳이: 5
# 생겼네요: 6
#
# 하지만 1부터 순서대로 붙는게 아니고
# "우리 동네에도 드디어 먹을만한 곳이 생겼네요" 데이터는 기존 학습 데이터와 일치하는 단어는 
# 해당 번호로 바뀌고 없는건 새로운 번호로 부여된다고 함. 그 처리를 하는 함수임.
test_sentence = "우리 동네에도 드디어 먹을만한 곳이 생겼네요"

# 그냥 숫자배열이라고 함
test_sequence = tokenizer.texts_to_sequences([test_sentence])

# 패딩 처리 (뒤로)
padded_test_sequence = pad_sequences(test_sequence, maxlen=max_len, padding='post')

prediction = model.predict(padded_test_sequence)

"""
[7/7] 예측 결과 출력
"""
# 학습데이터에서 긍정을 1로 했기때문에 
# prediction 값이 0.6 으로 나오면 1에 가까우니 긍정 으로 보면됨.
if prediction >= 0.5:
    print("긍정적인 문장입니다.")
else:
    print("부정적인 문장입니다.")
